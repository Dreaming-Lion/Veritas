# app/services/vector_store.py
from __future__ import annotations

from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import timezone
import logging
import time
import os

import joblib
from sklearn.feature_extraction.text import TfidfVectorizer

from qdrant_client import QdrantClient
from qdrant_client.http.models import (
    Distance,
    VectorParams,
    PointStruct,
    Filter,
    FieldCondition,
    Range,
    MatchValue,
)

logger = logging.getLogger(__name__)

VECTORIZER_PATH = Path("models/tfidf_news.pkl")

# Docker í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ Qdrant ì ‘ì†
QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION = "news_tfidf"

client = QdrantClient(
    host=QDRANT_HOST,
    port=QDRANT_PORT,
    timeout=1800.0,
)


def train_vectorizer_and_index_all(batch_size: int = 1000) -> Dict[str, int]:
    """
    news í…Œì´ë¸” ì „ì²´ë¥¼ ëŒ€ìƒìœ¼ë¡œ
      1) TF-IDF vectorizer í•™ìŠµ ë° ë””ìŠ¤í¬ ì €ì¥
      2) Qdrantì— ë²¡í„° + ë©”íƒ€ë°ì´í„° 'ë°°ì¹˜ ì—…ì„œíŠ¸'

    batch_size ë¡œ Qdrant upsertë¥¼ ìª¼ê°œì„œ íƒ€ì„ì•„ì›ƒ/ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì¸ë‹¤.

    âš ï¸ ë§¤ë²ˆ í˜¸ì¶œ ì‹œ Qdrant ì»¬ë ‰ì…˜(news_tfidf)ì„ dimì— ë§ê²Œ recreate_collectionìœ¼ë¡œ
       'í•­ìƒ' ìƒˆë¡œ ë§Œë“ ë‹¤. (dim ê¼¬ì„ ë°©ì§€)
    """
    # ìˆœí™˜ import í”¼í•˜ë ¤ê³  í•¨ìˆ˜ ì•ˆì—ì„œ import
    from app.services.recommend_core import get_conn

    t_global_start = time.time()

    logger.info("[vector_store] step1: news ì „ì²´ SELECT ì‹œì‘")

    conn = get_conn()
    cur = conn.cursor()
    try:
        cur.execute(
            """
            SELECT id, title, COALESCE(content, summary, ''), link, source, lean, date
            FROM news
            WHERE link IS NOT NULL
              AND link <> '';
            """
        )
        rows = cur.fetchall()
    finally:
        cur.close()
        conn.close()

    if not rows:
        logger.info("[vector_store] step1: news row ì—†ìŒ, ì¸ë±ì‹± ìŠ¤í‚µ")
        return {"indexed": 0}

    ids = [r[0] for r in rows]
    titles = [r[1] for r in rows]
    texts = [r[2] for r in rows]
    links = [r[3] for r in rows]
    srcs = [r[4] for r in rows]
    leans = [r[5] for r in rows]
    dates = [r[6] for r in rows]

    step1_time = time.time() - t_global_start
    logger.info(
        "[vector_store] step1 ì™„ë£Œ: rows=%d (%.2fì´ˆ ì†Œìš”)",
        len(rows),
        step1_time,
    )

    logger.info("[vector_store] step2: TF-IDF fit_transform ì‹œì‘")

    docs = [(t or "") + " " + ((x or "")[:1500]) for t, x in zip(titles, texts)]

    tfidf = TfidfVectorizer(min_df=2, ngram_range=(1, 2))
    X = tfidf.fit_transform(docs)

    VECTORIZER_PATH.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(tfidf, VECTORIZER_PATH)

    dim = X.shape[1]
    step2_time = time.time() - t_global_start - step1_time
    logger.info(
        "[vector_store] step2 ì™„ë£Œ: dim=%d (TF-IDF í•™ìŠµ %.2fì´ˆ ì†Œìš”)",
        dim,
        step2_time,
    )

    # ğŸ”¥ ë§¤ë²ˆ ì»¬ë ‰ì…˜ì„ dimì— ë§ê²Œ ìƒˆë¡œ ìƒì„± (dim ê¼¬ì„ ë°©ì§€)
    logger.info(
        "[vector_store] ì»¬ë ‰ì…˜ %s ë¥¼ dim=%d ë¡œ recreate_collection í•©ë‹ˆë‹¤.",
        COLLECTION,
        dim,
    )
    client.recreate_collection(
        collection_name=COLLECTION,
        vectors_config=VectorParams(
            size=dim,
            distance=Distance.COSINE,
        ),
    )

    total = len(ids)
    logger.info(
        "[vector_store] step3: Qdrant upsert ì‹œì‘ (total=%d, batch_size=%d)",
        total,
        batch_size,
    )

    indexed = 0

    for start in range(0, total, batch_size):
        end = min(start + batch_size, total)
        logger.info(
            "[vector_store] step3: upsert batch %d ~ %d / %d",
            start + 1,
            end,
            total,
        )

        X_batch = X[start:end].toarray().tolist()
        points: List[PointStruct] = []

        for offset, (doc_id, title, text, link, src, lean, dt) in enumerate(
            zip(
                ids[start:end],
                titles[start:end],
                texts[start:end],
                links[start:end],
                srcs[start:end],
                leans[start:end],
                dates[start:end],
            )
        ):
            vec = X_batch[offset]

            if dt is not None:
                dt_utc = dt
                if dt_utc.tzinfo is None:
                    dt_utc = dt_utc.replace(tzinfo=timezone.utc)
                ts = int(dt_utc.timestamp())
                date_iso = dt_utc.isoformat()
            else:
                ts = None
                date_iso = None

            payload: Dict[str, Any] = {
                "id": int(doc_id),
                "title": title,
                "content": text,
                "link": link,
                "source": src,
                "lean": lean,
                "date_ts": ts,
                "date": date_iso,
            }
            points.append(PointStruct(id=int(doc_id), vector=vec, payload=payload))

        logger.info(
            "[vector_store] step3: calling client.upsert() for batch %d ~ %d (count=%d)",
            start + 1,
            end,
            len(points),
        )
        try:
            client.upsert(collection_name=COLLECTION, points=points)
        except Exception as e:
            import traceback
            logger.error(
                "[vector_store] Qdrant upsert ì‹¤íŒ¨ (batch %d ~ %d): %r",
                start + 1,
                end,
                e,
            )
            traceback.print_exc()
            raise
        indexed += len(points)
        logger.info(
            "[vector_store] step3: done upsert batch %d ~ %d (ëˆ„ì  indexed=%d)",
            start + 1,
            end,
            indexed,
        )

    total_time = time.time() - t_global_start
    logger.info(
        "[vector_store] ì „ì²´ TF-IDF ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì™„ë£Œ: indexed=%d (ì´ %.2fì´ˆ)",
        indexed,
        total_time,
    )

    # Qdrant ì»¬ë ‰ì…˜ ì‹¤ì œ í¬ì¸íŠ¸ ê°œìˆ˜ë„ í•œ ë²ˆ ì°ê¸°
    try:
        count_res = client.count(collection_name=COLLECTION, exact=True)
        logger.info(
            "[vector_store] Qdrant ì»¬ë ‰ì…˜ %s í˜„ì¬ í¬ì¸íŠ¸ ê°œìˆ˜: %d",
            COLLECTION,
            count_res.count,
        )
    except Exception as e:
        logger.warning("[vector_store] Qdrant í¬ì¸íŠ¸ ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: %s", e)

    return {"indexed": indexed}


def load_vectorizer() -> TfidfVectorizer:
    """
    ë””ìŠ¤í¬ì—ì„œ TF-IDF vectorizer ë¡œë“œ.
    ì—†ìœ¼ë©´ ì—ëŸ¬ ë°œìƒ (ë¨¼ì € train_vectorizer_and_index_all ì‹¤í–‰ í•„ìš”).
    """
    if not VECTORIZER_PATH.exists():
        raise RuntimeError(
            "TF-IDF vectorizer not found. "
            "ë¨¼ì € train_vectorizer_and_index_all()ì„ ì‹¤í–‰í•´ì„œ ë²¡í„°ë¼ì´ì €ë¥¼ í•™ìŠµ/ì €ì¥í•˜ì„¸ìš”."
        )
    return joblib.load(VECTORIZER_PATH)


def _opposite_lean_values(base: Optional[str]) -> List[str]:
    """
    ê¸°ì¤€ leanì— ë”°ë¼ 'ë°˜ëŒ€í¸' ì„±í–¥ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜
    """
    if base == "progressive":
        return ["conservative"]
    if base == "conservative":
        return ["progressive"]
    if base == "centrist":
        return ["progressive", "conservative"]
    return []


def search_similar_with_lean_fallback(
    query_text: str,
    *,
    base_lean: Optional[str],
    base_date,
    hours_window: int,
    top_k: int = 50,
):
    """
    TF-IDF ì¿¼ë¦¬ ë²¡í„° ê¸°ë°˜ìœ¼ë¡œ Qdrantì—ì„œ ìœ ì‚¬ ê¸°ì‚¬ ê²€ìƒ‰.

    1) base_date ê¸°ì¤€ ì‹œê°„ì°½(hours_window) ì•ˆì—ì„œ ë¨¼ì € í•„í„°ë§
    2) ê¸°ì¤€ ê¸°ì‚¬ ì •ì¹˜ ì„±í–¥(base_lean)ì˜ 'ë°˜ëŒ€í¸' leanë§Œ ìš°ì„  ê²€ìƒ‰
       - ë°˜ëŒ€ lean ê²°ê³¼ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ lean ì œí•œ ì—†ì´ ì „ì²´ ê²€ìƒ‰
    """
    tfidf = load_vectorizer()
    q_vec = tfidf.transform([query_text]).toarray()[0].tolist()

    must_conditions: List[FieldCondition] = []

    # ì‹œê°„ í•„í„° (date_ts ë²”ìœ„)
    if base_date is not None:
        dt_utc = base_date
        if dt_utc.tzinfo is None:
            dt_utc = dt_utc.replace(tzinfo=timezone.utc)
        ts_center = int(dt_utc.timestamp())
        ts_from = ts_center - hours_window * 3600
        ts_to = ts_center + hours_window * 3600

        must_conditions.append(
            FieldCondition(
                key="date_ts",
                range=Range(gte=ts_from, lte=ts_to),
            )
        )

    # 1) ë¨¼ì € ë°˜ëŒ€ leanë§Œ ëŒ€ìƒìœ¼ë¡œ ê²€ìƒ‰ ì‹œë„
    opp_leans = _opposite_lean_values(base_lean)
    if opp_leans:
        lean_conds = [
            FieldCondition(key="lean", match=MatchValue(value=val))
            for val in opp_leans
        ]
        flt_opp = Filter(
            must=must_conditions,
            should=lean_conds,
        )

        res = client.query_points(
            collection_name=COLLECTION,
            query=q_vec,
            query_filter=flt_opp,
            limit=top_k,
            with_payload=True,
        )
        hits = res.points
        if hits:
            return hits

    # 2) ë°˜ëŒ€ lean ê²°ê³¼ê°€ ì—†ìœ¼ë©´ lean ì œí•œ ì—†ì´ ì „ì²´ ê²€ìƒ‰
    flt_all = Filter(must=must_conditions) if must_conditions else None
    res_all = client.query_points(
        collection_name=COLLECTION,
        query=q_vec,
        query_filter=flt_all,
        limit=top_k,
        with_payload=True,
    )
    return res_all.points
